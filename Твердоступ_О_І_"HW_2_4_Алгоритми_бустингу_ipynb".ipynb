{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tverdostup1993/Python_for_ds_tasks/blob/main/%D0%A2%D0%B2%D0%B5%D1%80%D0%B4%D0%BE%D1%81%D1%82%D1%83%D0%BF_%D0%9E_%D0%86_%22HW_2_4_%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D0%B8_%D0%B1%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3%D1%83_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В цьому домашньому завданні ми знову працюємо з даними з нашого змагання [\"Bank Customer Churn Prediction (DLU Course)\"](https://www.kaggle.com/t/7c080c5d8ec64364a93cf4e8f880b6a0).\n",
        "\n",
        "Тут ми побудуємо рішення задачі класифікації з використанням алгоритмів бустингу: XGBoost та LightGBM, а також використаємо бібліотеку HyperOpt для оптимізації гіперпараметрів."
      ],
      "metadata": {
        "id": "fDefDHQt8LXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. Зчитайте дані `train.csv` в змінну `raw_df` та скористайтесь наведеним кодом нижче аби розділити дані на трнувальні та валідаційні і розділити дані на ознаки з матириці Х та цільову змінну. Назви змінних `train_inputs, train_targets, train_inputs, train_targets` можна змінити на ті, які Вам зручно.\n",
        "\n",
        "  Наведений скрипт - частина отриманого мною скрипта для обробки даних. Ми тут не викнуємо масштабування та обробку категоріальних змінних, бо хочемо це делегувати алгоритмам, які будемо використовувати. Якщо щось не розумієте в наведених скриптах, рекомендую розібратись: навичка читати код - важлива складова роботи в машинному навчанні."
      ],
      "metadata": {
        "id": "LhivzW9W8-Dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Tuple, Dict, Any\n",
        "\n",
        "\n",
        "def split_train_val(df: pd.DataFrame, target_col: str, test_size: float = 0.2, random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split the dataframe into training and validation sets.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw dataframe.\n",
        "        target_col (str): The target column for stratification.\n",
        "        test_size (float): The proportion of the dataset to include in the validation split.\n",
        "        random_state (int): Random state for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]: Training and validation dataframes.\n",
        "    \"\"\"\n",
        "    train_df, val_df = train_test_split(df, test_size=test_size, random_state=random_state, stratify=df[target_col])\n",
        "    return train_df, val_df\n",
        "\n",
        "\n",
        "def separate_inputs_targets(df: pd.DataFrame, input_cols: list, target_col: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    Separate inputs and targets from the dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe.\n",
        "        input_cols (list): List of input columns.\n",
        "        target_col (str): Target column.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.Series]: DataFrame of inputs and Series of targets.\n",
        "    \"\"\"\n",
        "    inputs = df[input_cols].copy()\n",
        "    targets = df[target_col].copy()\n",
        "    return inputs, targets"
      ],
      "metadata": {
        "id": "cKE8RTPf6CRD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "TARGET_COL = \"Exited\"\n",
        "INPUT_COLS = [\n",
        "    \"CreditScore\", \"Geography\", \"Gender\", \"Age\", \"Tenure\",\n",
        "    \"Balance\", \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\", \"EstimatedSalary\"\n",
        "]\n",
        "\n",
        "train_df, val_df = split_train_val(raw_df, target_col=TARGET_COL)\n",
        "\n",
        "X_train, y_train = separate_inputs_targets(train_df, INPUT_COLS, TARGET_COL)\n",
        "X_val, y_val     = separate_inputs_targets(val_df, INPUT_COLS, TARGET_COL)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)\n"
      ],
      "metadata": {
        "id": "-bHdMJVB4xQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d48a78e-2c5b-4dfe-abb1-00146c670a3f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12000, 10) (12000,)\n",
            "(3000, 10) (3000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. В тренувальному та валідаційному наборі перетворіть категоріальні ознаки на тип `category`. Можна це зробити двома способами:\n",
        " 1. `df[col_name].astype('category')`, як було продемонстровано в лекції\n",
        " 2. використовуючи метод `pd.Categorical(df[col_name])`"
      ],
      "metadata": {
        "id": "cq0JU7MqHgp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = [\"Geography\", \"Gender\"]\n",
        "\n",
        "for col in cat_cols:\n",
        "    train_df[col] = train_df[col].astype(\"category\")\n",
        "    val_df[col]   = val_df[col].astype(\"category\")"
      ],
      "metadata": {
        "id": "UPmqo-Mr4yUO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syU40-Clrvmp",
        "outputId": "f86a19e6-0887-4667-b0a8-b3199cf9d26c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id                    int64\n",
            "CustomerId          float64\n",
            "Surname              object\n",
            "CreditScore         float64\n",
            "Geography          category\n",
            "Gender             category\n",
            "Age                 float64\n",
            "Tenure              float64\n",
            "Balance             float64\n",
            "NumOfProducts       float64\n",
            "HasCrCard           float64\n",
            "IsActiveMember      float64\n",
            "EstimatedSalary     float64\n",
            "Exited              float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for df_ in (train_df, val_df):\n",
        "    df_[\"Exited\"] = df_[\"Exited\"].astype(int)\n",
        "\n",
        "INPUT_COLS = [\n",
        "    \"CreditScore\", \"Geography\", \"Gender\", \"Age\", \"Tenure\",\n",
        "    \"Balance\", \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\", \"EstimatedSalary\"\n",
        "]\n",
        "TARGET_COL = \"Exited\"\n",
        "\n",
        "X_train, y_train = separate_inputs_targets(train_df, INPUT_COLS, TARGET_COL)\n",
        "X_val,   y_val   = separate_inputs_targets(val_df,   INPUT_COLS, TARGET_COL)\n",
        "\n",
        "for c in [\"Geography\", \"Gender\"]:\n",
        "    X_train[c] = X_train[c].astype(\"category\")\n",
        "    X_val[c]   = X_val[c].astype(\"category\")"
      ],
      "metadata": {
        "id": "SlpN5sbbsrDp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvrme-2Tsw-h",
        "outputId": "e68494f0-52c0-45bb-9b01-211d7c322a16"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id                    int64\n",
            "CustomerId          float64\n",
            "Surname              object\n",
            "CreditScore         float64\n",
            "Geography          category\n",
            "Gender             category\n",
            "Age                 float64\n",
            "Tenure              float64\n",
            "Balance             float64\n",
            "NumOfProducts       float64\n",
            "HasCrCard           float64\n",
            "IsActiveMember      float64\n",
            "EstimatedSalary     float64\n",
            "Exited                int64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Навчіть на отриманих даних модель `XGBoostClassifier`. Параметри алгоритму встановіть на свій розсуд, ми далі будемо їх тюнити. Рекомендую тренувати не дуже складну модель.\n",
        "\n",
        "  Опис всіх конфігураційних параметрів XGBoostClassifier - тут https://xgboost.readthedocs.io/en/stable/parameter.html#global-config\n",
        "\n",
        "  **Важливо:** зробіть такі налаштування `XGBoostClassifier` аби він самостійно обробляв незаповнені значення в даних і обробляв категоріальні колонки.\n",
        "\n",
        "  Можна також, якщо працюєте в Google Colab, увімкнути можливість використання GPU (`Runtime -> Change runtime type -> T4 GPU`) і встановити параметр `device='cuda'` в `XGBoostClassifier` для пришвидшення тренування бустинг моделі.\n",
        "  \n",
        "  Після тренування моделі\n",
        "  1. Виміряйте точність з допомогою AUROC на тренувальному та валідаційному наборах.\n",
        "  2. Зробіть висновок про отриману модель: вона хороша/погана, чи є high bias/high variance?\n",
        "  3. Порівняйте якість цієї моделі з тою, що ви отрмали з використанням DecisionTrees раніше. Чи вийшло покращити якість?"
      ],
      "metadata": {
        "id": "_LxWkv4o-wMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from typing import Tuple, List\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=4,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=1.0,\n",
        "    min_child_weight=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    tree_method=\"hist\",\n",
        "    enable_categorical=True,\n",
        "    eval_metric=\"auc\",\n",
        "    #device=\"cuda\",\n",
        ")\n",
        "\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "train_proba = xgb.predict_proba(X_train)[:, 1]\n",
        "val_proba   = xgb.predict_proba(X_val)[:, 1]\n",
        "\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "val_auc   = roc_auc_score(y_val,   val_proba)\n",
        "\n",
        "print(f\"XGBoost AUROC — train: {train_auc:.4f} | val: {val_auc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_5rDqdDP41hb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca8d498-1bbd-40c3-f315-db2feaa64963"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost AUROC — train: 0.9568 | val: 0.9353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train_enc = pd.get_dummies(X_train, drop_first=True)\n",
        "X_val_enc   = pd.get_dummies(X_val, drop_first=True)\n",
        "\n",
        "\n",
        "X_val_enc = X_val_enc.reindex(columns=X_train_enc.columns, fill_value=0)\n"
      ],
      "metadata": {
        "id": "KFRb3mdKubOt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dt = DecisionTreeClassifier(\n",
        "    max_depth=5, random_state=42\n",
        ")\n",
        "dt.fit(X_train_enc, y_train)\n",
        "dt_train_auc = roc_auc_score(y_train, dt.predict_proba(X_train_enc)[:, 1])\n",
        "dt_val_auc   = roc_auc_score(y_val,   dt.predict_proba(X_val_enc)[:, 1])\n",
        "print(f\"DecisionTree AUROC — train: {dt_train_auc:.4f} | val: {dt_val_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX2CQgLmraWm",
        "outputId": "1b662ded-e49f-4e17-9667-366b653a157d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTree AUROC — train: 0.9257 | val: 0.9219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отримана модель XGBoost показує високу якість: AUROC на тренувальних даних становить 0.9568, а на валідаційних — 0.9353, що свідчить про добру здатність до узагальнення та лише незначний розрив між train і val, тобто модель має легкий variance. У порівнянні з Decision Tree (train AUROC = 0.9257, val AUROC = 0.9219), XGBoost дає вищі результати, отже перехід до ансамблевого методу дозволив покращити якість моделі. Але все таки модель потребує доопрацювання"
      ],
      "metadata": {
        "id": "oj1RPT4yvrP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Використовуючи бібліотеку `Hyperopt` і приклад пошуку гіперпараметрів для `XGBoostClassifier` з лекції знайдіть оптимальні значення гіперпараметрів `XGBoostClassifier` для нашої задачі. Задайте свою сітку гіперпараметрів виходячи з тих параметрів, які ви б хотіли перебрати. Поставте кількість раундів в підборі гіперпараметрів рівну **20**.\n",
        "\n",
        "  **Увага!** Для того, аби скористатись hyperopt, нам треба задати функцію `objective`. В ній ми маємо задати loss - це може будь-яка метрика, але бажано використовувтаи ту, яка цільова в вашій задачі. Чим менший лосс - тим ліпша модель на думку hyperopt. Тож, тут нам треба задати loss - негативне значення AUROC. В лекції ми натомість використовували Accuracy.\n",
        "\n",
        "  Після успішного завершення пошуку оптимальних гіперпараметрів\n",
        "    - виведіть найкращі значення гіперпараметрів\n",
        "    - створіть в окремій зміній `final_clf` модель `XGBoostClassifier` з найкращими гіперпараметрами\n",
        "    - навчіть модель `final_clf`\n",
        "    - оцініть якість моделі `final_clf` на тренувальній і валідаційній вибірках з допомогою AUROC.\n",
        "    - зробіть висновок про якість моделі. Чи стала вона краще порівняно з попереднім пунктом (2) цього завдання?"
      ],
      "metadata": {
        "id": "U4hm5qYs_f7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "space = {\n",
        "    'n_estimators':     hp.quniform('n_estimators', 200, 800, 50),\n",
        "    'learning_rate':    hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
        "    'max_depth':        hp.quniform('max_depth', 3, 7, 1),\n",
        "    'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
        "    'subsample':        hp.uniform('subsample', 0.6, 1.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0),\n",
        "    'gamma':            hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
        "    'reg_alpha':        hp.loguniform('reg_alpha', np.log(1e-8), np.log(1.0)),\n",
        "    'reg_lambda':       hp.loguniform('reg_lambda', np.log(0.5), np.log(10.0)),\n",
        "}\n",
        "\n",
        "def objective(params):\n",
        "    params = params.copy()\n",
        "    params['n_estimators']     = int(params['n_estimators'])\n",
        "    params['max_depth']        = int(params['max_depth'])\n",
        "    params['min_child_weight'] = int(params['min_child_weight'])\n",
        "\n",
        "    clf = xgb.XGBClassifier(\n",
        "        n_estimators=params['n_estimators'],\n",
        "        learning_rate=params['learning_rate'],\n",
        "        max_depth=params['max_depth'],\n",
        "        min_child_weight=params['min_child_weight'],\n",
        "        subsample=params['subsample'],\n",
        "        colsample_bytree=params['colsample_bytree'],\n",
        "        gamma=params['gamma'],\n",
        "        reg_alpha=params['reg_alpha'],\n",
        "        reg_lambda=params['reg_lambda'],\n",
        "        enable_categorical=True,\n",
        "        tree_method='hist',\n",
        "        eval_metric='auc',\n",
        "        use_label_encoder=False,\n",
        "        missing=np.nan,\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        # device='cuda',\n",
        "        early_stopping_rounds=50\n",
        "    )\n",
        "\n",
        "    clf.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    val_proba = clf.predict_proba(X_val)[:, 1]\n",
        "    val_auc = roc_auc_score(y_val, val_proba)\n",
        "    return {'loss': -val_auc, 'status': STATUS_OK}\n",
        "\n",
        "trials = Trials()\n",
        "best = fmin(\n",
        "    fn=objective,\n",
        "    space=space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=20,\n",
        "    rstate=np.random.default_rng(42)\n",
        ")\n",
        "\n",
        "best_cast = {\n",
        "    'n_estimators':     int(best['n_estimators']),\n",
        "    'learning_rate':    float(best['learning_rate']),\n",
        "    'max_depth':        int(best['max_depth']),\n",
        "    'min_child_weight': int(best['min_child_weight']),\n",
        "    'subsample':        float(best['subsample']),\n",
        "    'colsample_bytree': float(best['colsample_bytree']),\n",
        "    'gamma':            float(best['gamma']),\n",
        "    'reg_alpha':        float(best['reg_alpha']),\n",
        "    'reg_lambda':       float(best['reg_lambda']),\n",
        "}\n",
        "print(\"Найкращі гіперпараметри:\", best_cast)\n",
        "\n",
        "final_clf = xgb.XGBClassifier(\n",
        "    **best_cast,\n",
        "    enable_categorical=True,\n",
        "    tree_method='hist',\n",
        "    eval_metric='auc',\n",
        "    use_label_encoder=False,\n",
        "    missing=np.nan,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    device='cuda',\n",
        "    early_stopping_rounds=50\n",
        ")\n",
        "\n",
        "final_clf.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "train_auc = roc_auc_score(y_train, final_clf.predict_proba(X_train)[:, 1])\n",
        "val_auc   = roc_auc_score(y_val,   final_clf.predict_proba(X_val)[:, 1])\n",
        "print(f\"final_clf AUROC — train: {train_auc:.4f} | val: {val_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "WhR1g9B4433r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0122c10c-1cf5-4d3c-8d99-d85d5e6dd6da"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0%|          | 0/20 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:42] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 10%|█         | 2/20 [00:02<00:21,  1.20s/trial, best loss: -0.936778585636875]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 15%|█▌        | 3/20 [00:03<00:12,  1.32trial/s, best loss: -0.936778585636875]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:45] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 20%|██        | 4/20 [00:03<00:10,  1.49trial/s, best loss: -0.936778585636875]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:45] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 25%|██▌       | 5/20 [00:03<00:08,  1.82trial/s, best loss: -0.936778585636875]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:45] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 30%|███       | 6/20 [00:04<00:07,  1.96trial/s, best loss: -0.936778585636875]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 35%|███▌      | 7/20 [00:06<00:12,  1.01trial/s, best loss: -0.936778585636875]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 40%|████      | 8/20 [00:06<00:10,  1.14trial/s, best loss: -0.936778585636875]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 45%|████▌     | 9/20 [00:07<00:08,  1.32trial/s, best loss: -0.936778585636875]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 50%|█████     | 10/20 [00:08<00:07,  1.43trial/s, best loss: -0.936778585636875]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:50] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 55%|█████▌    | 11/20 [00:08<00:05,  1.79trial/s, best loss: -0.936778585636875]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:50] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 60%|██████    | 12/20 [00:08<00:03,  2.14trial/s, best loss: -0.936778585636875]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:50] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 65%|██████▌   | 13/20 [00:08<00:03,  2.16trial/s, best loss: -0.936778585636875]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:51] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 70%|███████   | 14/20 [00:09<00:02,  2.26trial/s, best loss: -0.936778585636875]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:51] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 75%|███████▌  | 15/20 [00:09<00:01,  2.54trial/s, best loss: -0.936778585636875]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:51] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 85%|████████▌ | 17/20 [00:10<00:00,  3.12trial/s, best loss: -0.9368598669318883]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:52] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:52] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 90%|█████████ | 18/20 [00:10<00:00,  2.28trial/s, best loss: -0.9368598669318883]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:52] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 20/20 [00:11<00:00,  1.75trial/s, best loss: -0.9368598669318883]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:53] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Найкращі гіперпараметри: {'n_estimators': 400, 'learning_rate': 0.06318442685528432, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.950099043659659, 'colsample_bytree': 0.6632855608018366, 'gamma': 0.28814557154898146, 'reg_alpha': 4.307983349647148e-07, 'reg_lambda': 9.639305506171514}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [16:04:53] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final_clf AUROC — train: 0.9413 | val: 0.9370\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результат не кращий, валідаційний AUROC майже не змінився (0.9370 проти 0.9353 раніше), зате train знизився з 0.9568 до 0.9413, тож модель стала менше перенавчається."
      ],
      "metadata": {
        "id": "ZtQdo-8k8WtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Навчіть на наших даних модель LightGBM. Параметри алгоритму встановіть на свій розсуд, ми далі будемо їх тюнити. Рекомендую тренувати не дуже складну модель.\n",
        "\n",
        "  Опис всіх конфігураційних параметрів LightGBM - тут https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
        "\n",
        "  **Важливо:** зробіть такі налаштування LightGBM аби він самостійно обробляв незаповнені значення в даних і обробляв категоріальні колонки.\n",
        "\n",
        "  Аби передати категоріальні колонки в LightGBM - необхідно виявити їх індекси і передати в параметрі `cat_feature=cat_feature_indexes`\n",
        "\n",
        "  Після тренування моделі\n",
        "  1. Виміряйте точність з допомогою AUROC на тренувальному та валідаційному наборах.\n",
        "  2. Зробіть висновок про отриману модель: вона хороша/погана, чи є high bias/high variance?\n",
        "  3. Порівняйте якість цієї моделі з тою, що ви отрмали з використанням XGBoostClassifier раніше. Чи вийшло покращити якість?"
      ],
      "metadata": {
        "id": "Vg77SVWrBBmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "cat_cols = [c for c in X_train.columns if str(X_train.dtypes[c]) == \"category\"]\n",
        "cat_idx  = [X_train.columns.get_loc(c) for c in cat_cols]\n",
        "\n",
        "lgbm = lgb.LGBMClassifier(\n",
        "    objective=\"binary\",\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=25,\n",
        "    max_depth=-1,\n",
        "    subsample=0.7,\n",
        "    colsample_bytree=0.7,\n",
        "    min_child_samples=20,\n",
        "    reg_lambda=1.0,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    device_type=\"gpu\"\n",
        ")\n",
        "\n",
        "lgbm.fit(\n",
        "    X_train, y_train,\n",
        "    categorical_feature=cat_idx\n",
        ")\n",
        "\n",
        "train_auc = roc_auc_score(y_train, lgbm.predict_proba(X_train)[:, 1])\n",
        "val_auc   = roc_auc_score(y_val,   lgbm.predict_proba(X_val)[:, 1])\n",
        "\n",
        "print(f\"LightGBM AUROC — train: {train_auc:.4f} | val: {val_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "C-9aZn4d45No",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcbdf60d-5569-4a00-e10f-fd992443ff2b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 7 dense feature groups (0.09 MB) transferred to GPU in 0.000644 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "LightGBM AUROC — train: 0.9734 | val: 0.9344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Великий розрив між train і val (~0.039) свідчить про помітне перенавчання — модель надто добре підлаштувалась під тренувальні дані. Отже, модель добра, але не краща за XGBoost на валідації"
      ],
      "metadata": {
        "id": "wAsycfVFCYL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Використовуючи бібліотеку `Hyperopt` і приклад пошуку гіперпараметрів для `LightGBM` з лекції знайдіть оптимальні значення гіперпараметрів `LightGBM` для нашої задачі. Задайте свою сітку гіперпараметрів виходячи з тих параметрів, які ви б хотіли перебрати. Поставте кількість раундів в підборі гіперпараметрів рівну **10**.\n",
        "\n",
        "  **Увага!** Для того, аби скористатись hyperopt, нам треба задати функцію `objective`. І тут ми також ставимо loss - негативне значення AUROC, як і при пошуці гіперпараметрів для XGBoost. До речі, можна спробувати написати код так, аби в objective передавати лише модель і не писати схожий код двічі :)\n",
        "\n",
        "  Після успішного завершення пошуку оптимальних гіперпараметрів\n",
        "    - виведіть найкращі значення гіперпараметрів\n",
        "    - створіть в окремій зміній `final_lgb_clf` модель `LightGBM` з найкращими гіперпараметрами\n",
        "    - навчіть модель `final_lgb_clf`\n",
        "    - оцініть якість моделі `final_lgb_clf` на тренувальній і валідаційній вибірках з допомогою AUROC.\n",
        "    - зробіть висновок про якість моделі. Чи стала вона краще порівняно з попереднім пунктом (4) цього завдання?"
      ],
      "metadata": {
        "id": "nCnkGD_sEW1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "cat_cols = X_train.select_dtypes(include=\"category\").columns.tolist()\n",
        "cat_idx  = [X_train.columns.get_loc(c) for c in cat_cols]\n",
        "\n",
        "pos = float(y_train.sum())\n",
        "neg = float(len(y_train) - y_train.sum())\n",
        "scale_pos_weight = (neg / pos) if pos > 0 else 1.0\n",
        "\n",
        "space = {\n",
        "    \"n_estimators\":      hp.quniform(\"n_estimators\", 200, 800, 50),\n",
        "    \"learning_rate\":     hp.loguniform(\"learning_rate\", np.log(0.01), np.log(0.2)),\n",
        "    \"num_leaves\":        hp.quniform(\"num_leaves\", 16, 64, 4),\n",
        "    \"max_depth\":         hp.choice(\"max_depth\", [-1, 4, 6, 8, 10, 12]),\n",
        "    \"min_child_samples\": hp.quniform(\"min_child_samples\", 10, 60, 5),\n",
        "    \"subsample\":         hp.uniform(\"subsample\", 0.7, 1.0),\n",
        "    \"bagging_freq\":      hp.choice(\"bagging_freq\", [0, 1]),\n",
        "    \"colsample_bytree\":  hp.uniform(\"colsample_bytree\", 0.7, 1.0),\n",
        "    \"lambda_l1\":         hp.loguniform(\"lambda_l1\", np.log(1e-8), np.log(1.0)),\n",
        "    \"lambda_l2\":         hp.loguniform(\"lambda_l2\", np.log(0.5), np.log(10.0)),\n",
        "    \"min_split_gain\":    hp.loguniform(\"min_split_gain\", np.log(1e-8), np.log(1.0)),\n",
        "}\n",
        "\n",
        "def objective(params):\n",
        "    params = params.copy()\n",
        "    params[\"n_estimators\"]      = int(params[\"n_estimators\"])\n",
        "    params[\"num_leaves\"]        = int(params[\"num_leaves\"])\n",
        "    params[\"min_child_samples\"] = int(params[\"min_child_samples\"])\n",
        "\n",
        "    clf = lgb.LGBMClassifier(\n",
        "        objective=\"binary\",\n",
        "        boosting_type=\"gbdt\",\n",
        "        metric=\"auc\",\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        n_estimators=params[\"n_estimators\"],\n",
        "        learning_rate=params[\"learning_rate\"],\n",
        "        num_leaves=params[\"num_leaves\"],\n",
        "        max_depth=params[\"max_depth\"],\n",
        "        min_child_samples=params[\"min_child_samples\"],\n",
        "        subsample=params[\"subsample\"],\n",
        "        bagging_freq=params[\"bagging_freq\"],\n",
        "        colsample_bytree=params[\"colsample_bytree\"],\n",
        "        lambda_l1=params[\"lambda_l1\"],\n",
        "        lambda_l2=params[\"lambda_l2\"],\n",
        "        min_split_gain=params[\"min_split_gain\"],\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "    )\n",
        "\n",
        "    callbacks = [\n",
        "        lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
        "        lgb.log_evaluation(period=0),\n",
        "    ]\n",
        "\n",
        "    clf.fit(\n",
        "        X_train, y_train,\n",
        "        categorical_feature=cat_idx,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    val_auc = roc_auc_score(y_val, clf.predict_proba(X_val)[:, 1])\n",
        "    return {\"loss\": -val_auc, \"status\": STATUS_OK}\n",
        "\n",
        "trials = Trials()\n",
        "best = fmin(\n",
        "    fn=objective,\n",
        "    space=space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=10,\n",
        "    rstate=np.random.default_rng(42)\n",
        ")\n",
        "best_cast = {\n",
        "    \"n_estimators\":      int(best[\"n_estimators\"]),\n",
        "    \"learning_rate\":     float(best[\"learning_rate\"]),\n",
        "    \"num_leaves\":        int(best[\"num_leaves\"]),\n",
        "    \"max_depth\":         [-1, 4, 6, 8, 10, 12][best[\"max_depth\"]],\n",
        "    \"min_child_samples\": int(best[\"min_child_samples\"]),\n",
        "    \"subsample\":         float(best[\"subsample\"]),\n",
        "    \"bagging_freq\":      [0, 1][best[\"bagging_freq\"]],\n",
        "    \"colsample_bytree\":  float(best[\"colsample_bytree\"]),\n",
        "    \"lambda_l1\":         float(best[\"lambda_l1\"]),\n",
        "    \"lambda_l2\":         float(best[\"lambda_l2\"]),\n",
        "    \"min_split_gain\":    float(best[\"min_split_gain\"]),\n",
        "}\n",
        "print(\"Найкращі гіперпараметри:\", best_cast)\n",
        "\n",
        "final_lgb_clf = lgb.LGBMClassifier(\n",
        "    objective=\"binary\",\n",
        "    boosting_type=\"gbdt\",\n",
        "    metric=\"auc\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    **best_cast\n",
        ")\n",
        "final_lgb_clf.fit(\n",
        "    X_train, y_train,\n",
        "    categorical_feature=cat_idx,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    callbacks=[lgb.early_stopping(50, verbose=False), lgb.log_evaluation(0)]\n",
        ")\n",
        "\n",
        "train_auc = roc_auc_score(y_train, final_lgb_clf.predict_proba(X_train)[:, 1])\n",
        "val_auc   = roc_auc_score(y_val,   final_lgb_clf.predict_proba(X_val)[:, 1])\n",
        "print(f\"final_lgb_clf AUROC — train: {train_auc:.4f} | val: {val_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "cfMQKA4D47Rq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07434fa8-90ca-469c-dec6-f73c15c76b32"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.319782812839516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.319782812839516\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.47272923988995e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.47272923988995e-05\n",
            "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.319782812839516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.319782812839516\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.47272923988995e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.47272923988995e-05\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003302 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.319782812839516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.319782812839516\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.47272923988995e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.47272923988995e-05\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.319782812839516, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.319782812839516\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.47272923988995e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.47272923988995e-05\n",
            "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.5082882849937498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5082882849937498\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.432266870941544e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.432266870941544e-06\n",
            "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.5082882849937498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5082882849937498\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.432266870941544e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.432266870941544e-06\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000876 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.5082882849937498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5082882849937498\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.432266870941544e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.432266870941544e-06\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.5082882849937498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5082882849937498\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.432266870941544e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.432266870941544e-06\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.414976472336914, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.414976472336914\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0021256785517329816, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0021256785517329816\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.414976472336914, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.414976472336914\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0021256785517329816, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0021256785517329816\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001969 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.414976472336914, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.414976472336914\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0021256785517329816, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0021256785517329816\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.414976472336914, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.414976472336914\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0021256785517329816, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0021256785517329816\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.871852447158862, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.871852447158862\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0456444675309283e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0456444675309283e-06\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.871852447158862, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.871852447158862\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0456444675309283e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0456444675309283e-06\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009539 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.871852447158862, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.871852447158862\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0456444675309283e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0456444675309283e-06\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.871852447158862, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.871852447158862\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0456444675309283e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0456444675309283e-06\n",
            "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.895526675932177, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.895526675932177\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0026702120509158487, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0026702120509158487\n",
            "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.895526675932177, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.895526675932177\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0026702120509158487, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0026702120509158487\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008334 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.895526675932177, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.895526675932177\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0026702120509158487, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0026702120509158487\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.895526675932177, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.895526675932177\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0026702120509158487, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0026702120509158487\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.3811249111237265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.3811249111237265\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.07219861796340007, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.07219861796340007\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.3811249111237265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.3811249111237265\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.07219861796340007, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.07219861796340007\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007140 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.3811249111237265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.3811249111237265\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.07219861796340007, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.07219861796340007\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.3811249111237265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.3811249111237265\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.07219861796340007, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.07219861796340007\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.7452254058823506, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7452254058823506\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.8361318101849923e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.8361318101849923e-05\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.7452254058823506, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7452254058823506\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.8361318101849923e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.8361318101849923e-05\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007573 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.7452254058823506, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7452254058823506\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.8361318101849923e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.8361318101849923e-05\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.7452254058823506, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7452254058823506\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.8361318101849923e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.8361318101849923e-05\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.3598615644408323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3598615644408323\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.2007484418839023, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2007484418839023\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.3598615644408323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3598615644408323\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.2007484418839023, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2007484418839023\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004910 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.3598615644408323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3598615644408323\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.2007484418839023, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2007484418839023\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.3598615644408323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3598615644408323\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.2007484418839023, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2007484418839023\n",
            "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.303610069525526, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.303610069525526\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.082179271181588e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.082179271181588e-05\n",
            "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.303610069525526, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.303610069525526\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.082179271181588e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.082179271181588e-05\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000855 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.303610069525526, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.303610069525526\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.082179271181588e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.082179271181588e-05\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.303610069525526, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.303610069525526\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.082179271181588e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.082179271181588e-05\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.648938474863309, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648938474863309\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00010854060565745739, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00010854060565745739\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.648938474863309, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648938474863309\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00010854060565745739, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00010854060565745739\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004061 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.648938474863309, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648938474863309\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00010854060565745739, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00010854060565745739\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.648938474863309, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648938474863309\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00010854060565745739, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00010854060565745739\n",
            "100%|██████████| 10/10 [00:09<00:00,  1.10trial/s, best loss: -0.9355617669250291]\n",
            "Найкращі гіперпараметри: {'n_estimators': 550, 'learning_rate': 0.14434838103084277, 'num_leaves': 20, 'max_depth': 10, 'min_child_samples': 40, 'subsample': 0.7440888361855083, 'bagging_freq': 1, 'colsample_bytree': 0.8170211970323231, 'lambda_l1': 0.0021256785517329816, 'lambda_l2': 2.414976472336914, 'min_split_gain': 1.808995991393461e-07}\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.414976472336914, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.414976472336914\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0021256785517329816, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0021256785517329816\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.414976472336914, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.414976472336914\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0021256785517329816, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0021256785517329816\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000917 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.414976472336914, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.414976472336914\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0021256785517329816, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0021256785517329816\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.414976472336914, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.414976472336914\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0021256785517329816, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0021256785517329816\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.414976472336914, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.414976472336914\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0021256785517329816, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0021256785517329816\n",
            "final_lgb_clf AUROC — train: 0.9470 | val: 0.9356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Поточний LightGBM після Hyperopt дав найкращий результат: AUROC 0.9470 та 0.9356, що трохи вище за попередній LightGBM 0.9344 (+0.0012) і XGBoost 0.9353. Розрив між train і val невеликий, тож модель виглядає збалансованою"
      ],
      "metadata": {
        "id": "DKzYUb2MEdOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Оберіть модель з експериментів в цьому ДЗ і зробіть новий `submission` на Kaggle та додайте код для цього і скріншот скора на публічному лідерборді.\n",
        "  \n",
        "  **Напишіть коментар, чому ви обрали саме цю модель?**\n",
        "\n",
        "  І я вас вітаю - це останнє завдання з цим набором даних 💪 На цьому етапі корисно проаналізувати, які моделі показали себе найкраще і подумати, чому."
      ],
      "metadata": {
        "id": "XArADR2CG8VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обрала б  LightGBM (після Hyperopt) — він дав найкращу валідацію: AUROC 0.9356 (трохи вище за XGBoost 0.9353) і менший розрив train–val ≈0.011, тож модель більш збалансована й не перенавчається."
      ],
      "metadata": {
        "id": "l0aj1LG6Fhkq"
      }
    }
  ]
}